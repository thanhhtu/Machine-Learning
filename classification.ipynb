{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris.csv')\n",
    "\n",
    "# Encode label (species)\n",
    "label_map = {label: idx for idx, label in enumerate(data['species'].unique())}\n",
    "data['species'] = data['species'].map(label_map)\n",
    "\n",
    "mask = data['species'] < 2\n",
    "data = data[mask]\n",
    "# Chia dữ liệu thành input (X) và output (y)\n",
    "value = data.values\n",
    "X = value[:, 0:4]  # Input features (150, 5)\n",
    "y = value[:, -1]   # Output labels (150,)\n",
    "\n",
    "# Chia tập train/test\n",
    "train_x = X[:67, :]\n",
    "train_y = y[:67]\n",
    "test_x = X[67:100, :]\n",
    "test_y = y[67:100]\n",
    "\n",
    "# Hàm scale dữ liệu thủ công về khoảng [0, 1]\n",
    "def manual_min_max_scaler(data):\n",
    "    # Tìm min và max của từng cột (feature)\n",
    "    min_vals = np.min(data, axis=0)\n",
    "    max_vals = np.max(data, axis=0)\n",
    "    # Áp dụng công thức scale\n",
    "    scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "    return scaled_data\n",
    "\n",
    "# Scale dữ liệu\n",
    "train_x_scaled = manual_min_max_scaler(train_x)\n",
    "test_x_scaled = manual_min_max_scaler(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=1e-2, epoch=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.w = None\n",
    "        self.bias = 0\n",
    "        self.loss = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_cost(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        z = np.dot(X, self.w) + self.bias\n",
    "        s = self.sigmoid(z)\n",
    "        cost = (-1 / n) * np.sum(y * np.log(s) + (1 - y) * np.log(1 - s))\n",
    "        return cost\n",
    "\n",
    "    def compute_gradient(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        z = np.dot(X, self.w) + self.bias\n",
    "        s = self.sigmoid(z)\n",
    "        dw = (1 / n) * np.dot(X.T, (s - y))\n",
    "        db = (1 / n) * np.sum(s - y)\n",
    "        return dw, db\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        for i in range(self.epoch):\n",
    "            dw, db = self.compute_gradient(X, y)\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            cost = self.compute_cost(X, y)\n",
    "            self.loss.append(cost)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration: {i}, Cost = {cost}\")\n",
    "\n",
    "    def predict(self, x_test, threshold=0.5):\n",
    "        s = self.sigmoid(np.dot(x_test, self.w) + self.bias)\n",
    "        return s >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Cost = 0.6921365157378996\n",
      "Iteration: 100, Cost = 0.6108659627137228\n",
      "Iteration: 200, Cost = 0.5545559857986103\n",
      "Iteration: 300, Cost = 0.5111155616698746\n",
      "Iteration: 400, Cost = 0.4750891901221578\n",
      "Iteration: 500, Cost = 0.4439180895742659\n",
      "Iteration: 600, Cost = 0.41631244872781525\n",
      "Iteration: 700, Cost = 0.3915521153995611\n",
      "Iteration: 800, Cost = 0.3691826890007213\n",
      "Iteration: 900, Cost = 0.34888139678490376\n",
      "Iteration: 1000, Cost = 0.3303968903406982\n",
      "Iteration: 1100, Cost = 0.3135215283139004\n",
      "Iteration: 1200, Cost = 0.2980779991213439\n",
      "Iteration: 1300, Cost = 0.28391229741404994\n",
      "Iteration: 1400, Cost = 0.2708895493100522\n",
      "Iteration: 1500, Cost = 0.2588911605309163\n",
      "Iteration: 1600, Cost = 0.24781262953427738\n",
      "Iteration: 1700, Cost = 0.2375617430892751\n",
      "Iteration: 1800, Cost = 0.22805703077520395\n",
      "Iteration: 1900, Cost = 0.21922642048720198\n",
      "Iteration: 2000, Cost = 0.21100606327601368\n",
      "Iteration: 2100, Cost = 0.20333930611147097\n",
      "Iteration: 2200, Cost = 0.19617579529300072\n",
      "Iteration: 2300, Cost = 0.1894706952249525\n",
      "Iteration: 2400, Cost = 0.18318400864231604\n",
      "Iteration: 2500, Cost = 0.17727998561854114\n",
      "Iteration: 2600, Cost = 0.17172660993325234\n",
      "Iteration: 2700, Cost = 0.16649515261699252\n",
      "Iteration: 2800, Cost = 0.16155978368627474\n",
      "Iteration: 2900, Cost = 0.1568972342024762\n",
      "Iteration: 3000, Cost = 0.15248650181187168\n",
      "Iteration: 3100, Cost = 0.14830859384224926\n",
      "Iteration: 3200, Cost = 0.14434630284345923\n",
      "Iteration: 3300, Cost = 0.14058401016974484\n",
      "Iteration: 3400, Cost = 0.13700751381871654\n",
      "Iteration: 3500, Cost = 0.13360387727469114\n",
      "Iteration: 3600, Cost = 0.13036129656247009\n",
      "Iteration: 3700, Cost = 0.12726898311084162\n",
      "Iteration: 3800, Cost = 0.12431706036179298\n",
      "Iteration: 3900, Cost = 0.12149647234944663\n",
      "Iteration: 4000, Cost = 0.11879890271899887\n",
      "Iteration: 4100, Cost = 0.11621670286649316\n",
      "Iteration: 4200, Cost = 0.11374282806034622\n",
      "Iteration: 4300, Cost = 0.11137078055965563\n",
      "Iteration: 4400, Cost = 0.10909455887632291\n",
      "Iteration: 4500, Cost = 0.10690861244120395\n",
      "Iteration: 4600, Cost = 0.10480780103164529\n",
      "Iteration: 4700, Cost = 0.1027873584012646\n",
      "Iteration: 4800, Cost = 0.10084285962469165\n",
      "Iteration: 4900, Cost = 0.0989701917319194\n",
      "Iteration: 5000, Cost = 0.09716552726036928\n",
      "Iteration: 5100, Cost = 0.095425300398983\n",
      "Iteration: 5200, Cost = 0.09374618543865557\n",
      "Iteration: 5300, Cost = 0.09212507727801361\n",
      "Iteration: 5400, Cost = 0.09055907376366469\n",
      "Iteration: 5500, Cost = 0.08904545967024453\n",
      "Iteration: 5600, Cost = 0.08758169214841693\n",
      "Iteration: 5700, Cost = 0.08616538748889425\n",
      "Iteration: 5800, Cost = 0.08479430906795302\n",
      "Iteration: 5900, Cost = 0.0834663563551481\n",
      "Iteration: 6000, Cost = 0.08217955487728498\n",
      "Iteration: 6100, Cost = 0.08093204704442722\n",
      "Iteration: 6200, Cost = 0.07972208375402338\n",
      "Iteration: 6300, Cost = 0.07854801669831049\n",
      "Iteration: 6400, Cost = 0.07740829130815215\n",
      "Iteration: 6500, Cost = 0.07630144027353415\n",
      "Iteration: 6600, Cost = 0.07522607758718719\n",
      "Iteration: 6700, Cost = 0.07418089306334011\n",
      "Iteration: 6800, Cost = 0.07316464728850901\n",
      "Iteration: 6900, Cost = 0.07217616696558649\n",
      "Iteration: 7000, Cost = 0.07121434061636545\n",
      "Iteration: 7100, Cost = 0.0702781146110813\n",
      "Iteration: 7200, Cost = 0.06936648949662631\n",
      "Iteration: 7300, Cost = 0.06847851659783667\n",
      "Iteration: 7400, Cost = 0.06761329486870005\n",
      "Iteration: 7500, Cost = 0.06676996797252636\n",
      "Iteration: 7600, Cost = 0.06594772157208871\n",
      "Iteration: 7700, Cost = 0.06514578081250236\n",
      "Iteration: 7800, Cost = 0.06436340798119178\n",
      "Iteration: 7900, Cost = 0.06359990033071847\n",
      "Iteration: 8000, Cost = 0.06285458805151994\n",
      "Iteration: 8100, Cost = 0.06212683238276442\n",
      "Iteration: 8200, Cost = 0.06141602385056563\n",
      "Iteration: 8300, Cost = 0.06072158062373861\n",
      "Iteration: 8400, Cost = 0.06004294697812764\n",
      "Iteration: 8500, Cost = 0.05937959186130451\n",
      "Iteration: 8600, Cost = 0.058731007550127695\n",
      "Iteration: 8700, Cost = 0.05809670839428687\n",
      "Iteration: 8800, Cost = 0.057476229639525526\n",
      "Iteration: 8900, Cost = 0.05686912632475448\n",
      "Iteration: 9000, Cost = 0.05627497224774122\n",
      "Iteration: 9100, Cost = 0.05569335899448866\n",
      "Iteration: 9200, Cost = 0.05512389502780895\n",
      "Iteration: 9300, Cost = 0.054566204830952855\n",
      "Iteration: 9400, Cost = 0.05401992810248083\n",
      "Iteration: 9500, Cost = 0.053484718998860654\n",
      "Iteration: 9600, Cost = 0.052960245421543845\n",
      "Iteration: 9700, Cost = 0.052446188345525405\n",
      "Iteration: 9800, Cost = 0.05194224118661584\n",
      "Iteration: 9900, Cost = 0.051448109204863264\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.train(train_x_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      0.39      0.57        33\n",
      "\n",
      "    accuracy                           0.39        33\n",
      "   macro avg       0.50      0.20      0.28        33\n",
      "weighted avg       1.00      0.39      0.57        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_x_scaled)\n",
    "print(classification_report(test_y, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
