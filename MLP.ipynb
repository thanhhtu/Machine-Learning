{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data  \n",
    "y = mnist.target\n",
    "X = X.astype(np.float64)\n",
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot (y, num_class):\n",
    "    \"\"\"\n",
    "    Convert interger labels to one-hot labels\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    num_sample = y.shape[0]\n",
    "    one_hot = np.zeros((num_sample, num_class)) #số hàng là số sample, số cột là số class (số label)\n",
    "    one_hot[np.arange(num_sample), y] = 1 \n",
    "    return one_hot\n",
    "\n",
    "y_train, y_test = one_hot(y_train, 10), one_hot(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.b1 = np.zeros((1,self.hidden_size))\n",
    "        self.b2 = np.zeros((1,self.output_size)) \n",
    "        #pp chuẩn\n",
    "        # self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        # self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        #Xavier initialization\n",
    "        # self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(1 / self.input_size)\n",
    "        # self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(1 / self.hidden_size)\n",
    "\n",
    "        #He initialization\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2 / self.input_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2 / self.hidden_size)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z)* self.sigmoid(1-z) #shape (num_samples, hidden_size)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0,z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_z = np.exp(z-np.max(z))\n",
    "        return e_z/np.sum(e_z, axis = 1, keepdims = True)\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        return -np.mean(y_true*np.log(y_pred))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward method for neural network\n",
    "        args:\n",
    "        X (array): input array of shape (num_samples, 784)\n",
    "        returns:\n",
    "        output (array): output array of shape (num_samples, 10)\n",
    "        \"\"\"\n",
    "        # dùng sigmoid\n",
    "        # self.z1 = np.dot(X, self.W1) + self.b1 # shape (num_samples, hidden_size)\n",
    "        # self.a1 = self.sigmoid(self.z1) # shape (num_samples, hidden_size)  \n",
    "        # self.z2 = np.dot(self.a1, self.W2) + self.b2 # shape (num_samples, 10)  \n",
    "        # self.a2 = self.softmax(self.z2)\n",
    "\n",
    "        #dùng relu\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1 # shape (num_samples, hidden_size)\n",
    "        self.a1 = self.relu(self.z1) # shape (num_samples, hidden_size)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2 # shape (num_samples, 10)  \n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, y_pred):\n",
    "        dz2 = y_pred - y # shape (num_samples, 10)\n",
    "        dW2 = np.dot(self.a1.T, dz2) # shape (hidden_size, 10)\n",
    "        db2 = np.sum(dz2, axis = 0, keepdims = True)\n",
    "\n",
    "        #np.dot(dz2, self.W2.T) shape (num_samples, hidden_size)\n",
    "\n",
    "        #dùng sigmoid\n",
    "        # dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.z1)\n",
    "        # dW1 = np.dot(X.T, dz1)\n",
    "        # db1 = np.sum(dz1, axis = 0, keepdims = True)\n",
    "\n",
    "        #dùng relu\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.relu_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dz1)\n",
    "        db1 = np.sum(dz1, axis = 0, keepdims = True)\n",
    "        return  dW2, db2,dW1, db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis = 1) #trả về số của class có xác suất lớn nhất\n",
    "\n",
    "    def train(self, X ,y, epochs=101, lr=1e-3):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            dW2, db2, dW1, db1 = self.backward(X, y, y_pred)\n",
    "            self.W2 -= lr*dW2\n",
    "            self.b2 -= lr*db2\n",
    "            self.W1 -= lr*dW1\n",
    "            self.b1 -= lr*db1\n",
    "            if(epoch % 10 == 0):\n",
    "                print(f\"Loss at epoch {epoch} is {self.loss(y_pred, y)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 is 0.2906791746017246\n",
      "Loss at epoch 10 is 0.03986490209447911\n",
      "Loss at epoch 20 is 0.029854139091926073\n",
      "Loss at epoch 30 is 0.02518774063802912\n",
      "Loss at epoch 40 is 0.022238461094865037\n",
      "Loss at epoch 50 is 0.02011289540068013\n",
      "Loss at epoch 60 is 0.01846892265156566\n",
      "Loss at epoch 70 is 0.017141956879043466\n",
      "Loss at epoch 80 is 0.01603552840791515\n",
      "Loss at epoch 90 is 0.015090037427681054\n",
      "Loss at epoch 100 is 0.014269759724405803\n",
      "Loss at epoch 110 is 0.013547860529563336\n",
      "Loss at epoch 120 is 0.01290859374801702\n",
      "Loss at epoch 130 is 0.012337010098474006\n",
      "Loss at epoch 140 is 0.011818185867742435\n",
      "Loss at epoch 150 is 0.011345494889074407\n",
      "Loss at epoch 160 is 0.010911238412258228\n",
      "Loss at epoch 170 is 0.010508700045007055\n",
      "Loss at epoch 180 is 0.010134083790199935\n",
      "Loss at epoch 190 is 0.009784402187798906\n",
      "Loss at epoch 200 is 0.009456469116142075\n"
     ]
    }
   ],
   "source": [
    "nn= NeuralNetwork(input_size=784, hidden_size=100, output_size=10)\n",
    "nn.train(X_train, y_train, epochs=201, lr=1e-5)\n",
    "#tăng hidden_size từ 64 lên 100 thì bị nan \n",
    "# => thay đổi learning rate thì không bị nan nữa, nhưng accuracy chỉ có 0.87\n",
    "# => thay đổi learning rate và Xavier initialization cho W1, W2 thì accuracy tăng lên 0.94\n",
    "# => giảm lr, tăng epochs và sử dụng Xavier thì accuracy tăng lên 0.959\n",
    "\n",
    "\n",
    "# nếu dùng ReLU thì tiếp tục giảm lr và dùng He initialization thì accuracy là 0.959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9592142857142857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded y_test back to integer labels\n",
    "accuracy = accuracy_score(y_pred=y_pred, y_true=y_test_labels)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9591850880574626\n"
     ]
    }
   ],
   "source": [
    "#f1score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true=y_test_labels, y_pred=y_pred, average='weighted')\n",
    "\n",
    "print(\"F1-Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
