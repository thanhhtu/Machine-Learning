{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_iris()\n",
    "X, y = dataset.data, dataset.target\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4), (100,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = y < 2\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify = y # 50:50\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr = 1e-2, epoch = 10000):\n",
    "        \"\"\"\n",
    "        Initizes the logistic regression\n",
    "        Args:\n",
    "            lr: float, learning rate\n",
    "            epoch: int, number of epoch\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.weights = None \n",
    "        self.bias = None\n",
    "        self.loss = [] \n",
    "\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute sigmoid function\n",
    "        Args:\n",
    "            z(array):input value of shape (n_samples, )\n",
    "        Returns:\n",
    "            array: sigmoid of input value of shape (n_samples, )\n",
    "        \"\"\"\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"\"\n",
    "        Compute binary cross-entropy loss\n",
    "        Args:\n",
    "            X (array): input features of shape (n_samples, n_features)\n",
    "            y (array): target valus of shape (n_samples, )\n",
    "        Returns:\n",
    "            float: binary cross-entropy loss\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        s = self.sigmoid(z)\n",
    "        s = np.clip(s, 1e-15, 1 - 1e-15) # avoid log0\n",
    "        cost = (-1 / num_samples) * np.sum(y * np.log(s) + (1 - y) * np.log(1 - s))\n",
    "        return cost\n",
    "    \n",
    "    def compute_gradient(self,X,y):\n",
    "        \"\"\"\"\n",
    "        Compute gradient of the cost function\n",
    "        Args:\n",
    "            X (array): input features of shape (n_samples, n_features)\n",
    "            y (array): target valus of shape (n_samples, )\n",
    "        Returns:\n",
    "            tuple: gradient of weights and bias\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        s = self.sigmoid(z)\n",
    "        dw = (1 / num_samples) * np.dot(X.T, s-y)\n",
    "        db = (1 / num_samples) * np.sum(s - y)\n",
    "        return dw, db\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "        Args:\n",
    "            X (array): input features of shape (n_samples, n_features)\n",
    "            y (array): target valus of shape (n_samples, )\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "        # Gradient descent\n",
    "        for i in range(self.epoch + 1):\n",
    "            # Compute gradient\n",
    "            dw, db = self.compute_gradient(X, y)\n",
    "            # Update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "            # Print loss at every 100th epoch\n",
    "            if i % 100 == 0:\n",
    "                loss = self.compute_cost(X, y)\n",
    "                self.loss.append(loss)\n",
    "                print(f\"Epoch {i}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        Args:\n",
    "            X(array): input features of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            array: predicted values of shape (n_samples, )\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        return s >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6869227525215216\n",
      "Epoch 100, Loss: 0.3919876807139968\n",
      "Epoch 200, Loss: 0.26335082695983625\n",
      "Epoch 300, Loss: 0.1955577248572467\n",
      "Epoch 400, Loss: 0.15477202865907513\n",
      "Epoch 500, Loss: 0.12783999732921023\n",
      "Epoch 600, Loss: 0.10883257419317963\n",
      "Epoch 700, Loss: 0.0947419283602128\n",
      "Epoch 800, Loss: 0.08389660805250809\n",
      "Epoch 900, Loss: 0.07529942770733478\n",
      "Epoch 1000, Loss: 0.0683209302810859\n",
      "Epoch 1100, Loss: 0.06254519147114798\n",
      "Epoch 1200, Loss: 0.057686821487391016\n",
      "Epoch 1300, Loss: 0.05354371547375815\n",
      "Epoch 1400, Loss: 0.04996886059294166\n",
      "Epoch 1500, Loss: 0.04685282850677332\n",
      "Epoch 1600, Loss: 0.04411252386105618\n",
      "Epoch 1700, Loss: 0.04168373471708385\n",
      "Epoch 1800, Loss: 0.03951606990875181\n",
      "Epoch 1900, Loss: 0.03756943841052399\n",
      "Epoch 2000, Loss: 0.03581155042624604\n",
      "Epoch 2100, Loss: 0.03421611091756375\n",
      "Epoch 2200, Loss: 0.03276149201271555\n",
      "Epoch 2300, Loss: 0.03142974270686969\n",
      "Epoch 2400, Loss: 0.030205840094625964\n",
      "Epoch 2500, Loss: 0.029077116189499055\n",
      "Epoch 2600, Loss: 0.028032814161897585\n",
      "Epoch 2700, Loss: 0.02706374118102377\n",
      "Epoch 2800, Loss: 0.026161994211470658\n",
      "Epoch 2900, Loss: 0.02532074150105893\n",
      "Epoch 3000, Loss: 0.024534047007761735\n",
      "Epoch 3100, Loss: 0.023796728241855074\n",
      "Epoch 3200, Loss: 0.023104240337350486\n",
      "Epoch 3300, Loss: 0.022452580878805535\n",
      "Epoch 3400, Loss: 0.021838211276414336\n",
      "Epoch 3500, Loss: 0.021257991428785875\n",
      "Epoch 3600, Loss: 0.020709125126486445\n",
      "Epoch 3700, Loss: 0.020189114192154658\n",
      "Epoch 3800, Loss: 0.01969571976907441\n",
      "Epoch 3900, Loss: 0.0192269294915101\n",
      "Epoch 4000, Loss: 0.01878092952018368\n",
      "Epoch 4100, Loss: 0.018356080622173816\n",
      "Epoch 4200, Loss: 0.017950897628969488\n",
      "Epoch 4300, Loss: 0.01756403172892498\n",
      "Epoch 4400, Loss: 0.0171942551481155\n",
      "Epoch 4500, Loss: 0.016840447852015362\n",
      "Epoch 4600, Loss: 0.016501585963669813\n",
      "Epoch 4700, Loss: 0.016176731645296854\n",
      "Epoch 4800, Loss: 0.015865024232011436\n",
      "Epoch 4900, Loss: 0.015565672440524154\n",
      "Epoch 5000, Loss: 0.015277947503740941\n",
      "Epoch 5100, Loss: 0.015001177105356991\n",
      "Epoch 5200, Loss: 0.014734740007730668\n",
      "Epoch 5300, Loss: 0.014478061282289973\n",
      "Epoch 5400, Loss: 0.014230608065048318\n",
      "Epoch 5500, Loss: 0.013991885770974558\n",
      "Epoch 5600, Loss: 0.013761434710345345\n",
      "Epoch 5700, Loss: 0.013538827058127118\n",
      "Epoch 5800, Loss: 0.013323664134130005\n",
      "Epoch 5900, Loss: 0.013115573957362632\n",
      "Epoch 6000, Loss: 0.012914209042853076\n",
      "Epoch 6100, Loss: 0.012719244413332984\n",
      "Epoch 6200, Loss: 0.012530375801714885\n",
      "Epoch 6300, Loss: 0.012347318023327798\n",
      "Epoch 6400, Loss: 0.012169803499485899\n",
      "Epoch 6500, Loss: 0.01199758091621578\n",
      "Epoch 6600, Loss: 0.011830414003915286\n",
      "Epoch 6700, Loss: 0.011668080425403028\n",
      "Epoch 6800, Loss: 0.011510370761282626\n",
      "Epoch 6900, Loss: 0.011357087582821689\n",
      "Epoch 7000, Loss: 0.011208044603657768\n",
      "Epoch 7100, Loss: 0.011063065902614614\n",
      "Epoch 7200, Loss: 0.010921985210765883\n",
      "Epoch 7300, Loss: 0.010784645256628545\n",
      "Epoch 7400, Loss: 0.010650897164026403\n",
      "Epoch 7500, Loss: 0.01052059989774231\n",
      "Epoch 7600, Loss: 0.010393619752588304\n",
      "Epoch 7700, Loss: 0.010269829881974075\n",
      "Epoch 7800, Loss: 0.010149109862453355\n",
      "Epoch 7900, Loss: 0.010031345291082532\n",
      "Epoch 8000, Loss: 0.009916427412740052\n",
      "Epoch 8100, Loss: 0.00980425277483494\n",
      "Epoch 8200, Loss: 0.00969472290708268\n",
      "Epoch 8300, Loss: 0.009587744024248154\n",
      "Epoch 8400, Loss: 0.009483226749955316\n",
      "Epoch 8500, Loss: 0.009381085859839676\n",
      "Epoch 8600, Loss: 0.00928124004248068\n",
      "Epoch 8700, Loss: 0.009183611676691936\n",
      "Epoch 8800, Loss: 0.009088126623878074\n",
      "Epoch 8900, Loss: 0.008994714034279974\n",
      "Epoch 9000, Loss: 0.008903306166036318\n",
      "Epoch 9100, Loss: 0.008813838216081366\n",
      "Epoch 9200, Loss: 0.008726248161985249\n",
      "Epoch 9300, Loss: 0.00864047661391772\n",
      "Epoch 9400, Loss: 0.008556466675987293\n",
      "Epoch 9500, Loss: 0.008474163816268919\n",
      "Epoch 9600, Loss: 0.008393515744891325\n",
      "Epoch 9700, Loss: 0.008314472299606001\n",
      "Epoch 9800, Loss: 0.008236985338306977\n",
      "Epoch 9900, Loss: 0.008161008638013396\n",
      "Epoch 10000, Loss: 0.008086497799865117\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(epoch = 10000)\n",
    "model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_true = y_test, y_pred = y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
