{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Wine recognition dataset, 178 samples, 13 features\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178,))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_wine()\n",
    "X, y = dataset.data, dataset.target\n",
    "mask = y <2\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130, 13), (130,))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = X[mask], y[mask]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = [0]*65 + [1]*65\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=strat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=1e-2, epoch=10000):\n",
    "        \"\"\"\n",
    "        Initizes the logistic regression\n",
    "        Args:\n",
    "            lr: float, learning rate\n",
    "            epoch: int, number of epoch\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.weights = None \n",
    "        self.bias = None\n",
    "        self.loss=[] \n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute sigmoid function\n",
    "        Args:\n",
    "        z(array):input value of shape (n_samples,)\n",
    "        Returns:\n",
    "        array: sigmoid of input value of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"\"\n",
    "        Compute binary cross-entropy loss\n",
    "        Args:\n",
    "        X(array): input features of shape (n_samples, n_features)\n",
    "        y(array): Target valus of shape (n_samples,)\n",
    "        Returns:\n",
    "        float: binary cross-entropy loss\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        s = self.sigmoid(z)\n",
    "        s = np.clip(s, 1e-15, 1-1e-15)\n",
    "        cost = (-1/num_samples)*np.sum(y*np.log(s) + (1-y)*np.log(1-s))\n",
    "        return cost\n",
    "    \n",
    "    def compute_gradient(self,X,y):\n",
    "        \"\"\"\"\n",
    "        Compute gradient of the cost function\n",
    "        Args:\n",
    "        X(array): input features of shape (n_samples, n_features)\n",
    "        y(array): Target valus of shape (n_samples,)\n",
    "        Returns:\n",
    "        tuple: gradient of weights and bias\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        s = self.sigmoid(z)\n",
    "        dw = (1/num_samples)*np.dot(X.T, (s-y))\n",
    "        db = (1/num_samples)*np.sum(s-y)\n",
    "        return dw, db\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "        Args:\n",
    "        X(array): input features of shape (n_samples, n_features)\n",
    "        y(array): Target valus of shape (n_samples,)\n",
    "\n",
    "        \"\"\"\n",
    "        #Initialize parameters\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "        #Gradient descent\n",
    "        for i in range(self.epoch+1):\n",
    "            #Compute gradient\n",
    "            dw, db = self.compute_gradient(X, y)\n",
    "            #Update parameters\n",
    "            self.weights -= self.lr*dw\n",
    "            self.bias -= self.lr*db\n",
    "\n",
    "            #print loss at every 100th epoch\n",
    "            if i%100 == 0:\n",
    "                loss = self.compute_cost(X, y)\n",
    "                self.loss.append(loss)\n",
    "                print(f\"Epoch {i}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X,threshold=0.5):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        Args:\n",
    "        X(array): input features of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "        array: predicted valus of shape (n_samples,)\n",
    "        \"\"\"\n",
    "\n",
    "        s= self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        return s>=threshold # quy định luôn là lớn hơn 0.5 thì là 1, nhỏ hơn 0.5 thì là 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6848736739559116\n",
      "Epoch 100, Loss: 0.3273874445643366\n",
      "Epoch 200, Loss: 0.23379971729920307\n",
      "Epoch 300, Loss: 0.18829159628285916\n",
      "Epoch 400, Loss: 0.16009038086723426\n",
      "Epoch 500, Loss: 0.14041326136178825\n",
      "Epoch 600, Loss: 0.12570233552316268\n",
      "Epoch 700, Loss: 0.11419524879928711\n",
      "Epoch 800, Loss: 0.10489959409824146\n",
      "Epoch 900, Loss: 0.09720546150841414\n",
      "Epoch 1000, Loss: 0.09071372974730652\n",
      "Epoch 1100, Loss: 0.08515062034776762\n",
      "Epoch 1200, Loss: 0.08032130819294267\n",
      "Epoch 1300, Loss: 0.07608298807238165\n",
      "Epoch 1400, Loss: 0.07232838893819993\n",
      "Epoch 1500, Loss: 0.06897524506689998\n",
      "Epoch 1600, Loss: 0.06595933275704825\n",
      "Epoch 1700, Loss: 0.06322972834635744\n",
      "Epoch 1800, Loss: 0.06074549726481598\n",
      "Epoch 1900, Loss: 0.058473331664593856\n",
      "Epoch 2000, Loss: 0.05638583249852157\n",
      "Epoch 2100, Loss: 0.054460238965335694\n",
      "Epoch 2200, Loss: 0.052677474493330025\n",
      "Epoch 2300, Loss: 0.0510214205479324\n",
      "Epoch 2400, Loss: 0.049478356954312554\n",
      "Epoch 2500, Loss: 0.04803652563656833\n",
      "Epoch 2600, Loss: 0.046685787004328586\n",
      "Epoch 2700, Loss: 0.04541734670773355\n",
      "Epoch 2800, Loss: 0.04422353641907544\n",
      "Epoch 2900, Loss: 0.04309763651059179\n",
      "Epoch 3000, Loss: 0.04203373152395251\n",
      "Epoch 3100, Loss: 0.04102659152775433\n",
      "Epoch 3200, Loss: 0.040071574077900275\n",
      "Epoch 3300, Loss: 0.039164542698587296\n",
      "Epoch 3400, Loss: 0.03830179870426981\n",
      "Epoch 3500, Loss: 0.03748002386654177\n",
      "Epoch 3600, Loss: 0.03669623195200528\n",
      "Epoch 3700, Loss: 0.035947727559227205\n",
      "Epoch 3800, Loss: 0.035232070994814825\n",
      "Epoch 3900, Loss: 0.03454704817239799\n",
      "Epoch 4000, Loss: 0.03389064471009177\n",
      "Epoch 4100, Loss: 0.033261023553880505\n",
      "Epoch 4200, Loss: 0.032656505575358444\n",
      "Epoch 4300, Loss: 0.03207555268921764\n",
      "Epoch 4400, Loss: 0.031516753114000456\n",
      "Epoch 4500, Loss: 0.03097880846291398\n",
      "Epoch 4600, Loss: 0.030460522403020274\n",
      "Epoch 4700, Loss: 0.029960790663254733\n",
      "Epoch 4800, Loss: 0.02947859220634876\n",
      "Epoch 4900, Loss: 0.029012981408310334\n",
      "Epoch 5000, Loss: 0.028563081112796974\n",
      "Epoch 5100, Loss: 0.02812807644742077\n",
      "Epoch 5200, Loss: 0.027707209305484773\n",
      "Epoch 5300, Loss: 0.027299773410446388\n",
      "Epoch 5400, Loss: 0.02690510989201278\n",
      "Epoch 5500, Loss: 0.026522603312571447\n",
      "Epoch 5600, Loss: 0.026151678090958266\n",
      "Epoch 5700, Loss: 0.02579179527761565\n",
      "Epoch 5800, Loss: 0.025442449641201532\n",
      "Epoch 5900, Loss: 0.02510316703184452\n",
      "Epoch 6000, Loss: 0.024773501990640615\n",
      "Epoch 6100, Loss: 0.024453035578769263\n",
      "Epoch 6200, Loss: 0.02414137340286411\n",
      "Epoch 6300, Loss: 0.023838143816088314\n",
      "Epoch 6400, Loss: 0.02354299627680189\n",
      "Epoch 6500, Loss: 0.02325559984882294\n",
      "Epoch 6600, Loss: 0.022975641829125218\n",
      "Epoch 6700, Loss: 0.02270282649041874\n",
      "Epoch 6800, Loss: 0.022436873927461812\n",
      "Epoch 6900, Loss: 0.022177518997180286\n",
      "Epoch 7000, Loss: 0.021924510343745974\n",
      "Epoch 7100, Loss: 0.021677609500713967\n",
      "Epoch 7200, Loss: 0.021436590063150407\n",
      "Epoch 7300, Loss: 0.02120123692341938\n",
      "Epoch 7400, Loss: 0.020971345564946595\n",
      "Epoch 7500, Loss: 0.020746721408853877\n",
      "Epoch 7600, Loss: 0.020527179208868273\n",
      "Epoch 7700, Loss: 0.020312542490363995\n",
      "Epoch 7800, Loss: 0.020102643029798323\n",
      "Epoch 7900, Loss: 0.019897320371162902\n",
      "Epoch 8000, Loss: 0.0196964213763924\n",
      "Epoch 8100, Loss: 0.019499799806959536\n",
      "Epoch 8200, Loss: 0.019307315934142838\n",
      "Epoch 8300, Loss: 0.019118836175683395\n",
      "Epoch 8400, Loss: 0.018934232756753468\n",
      "Epoch 8500, Loss: 0.018753383393346017\n",
      "Epoch 8600, Loss: 0.0185761709963611\n",
      "Epoch 8700, Loss: 0.01840248339481638\n",
      "Epoch 8800, Loss: 0.018232213076743643\n",
      "Epoch 8900, Loss: 0.018065256946457844\n",
      "Epoch 9000, Loss: 0.01790151609699496\n",
      "Epoch 9100, Loss: 0.01774089559661578\n",
      "Epoch 9200, Loss: 0.017583304288364722\n",
      "Epoch 9300, Loss: 0.01742865460175421\n",
      "Epoch 9400, Loss: 0.017276862375721715\n",
      "Epoch 9500, Loss: 0.017127846692073446\n",
      "Epoch 9600, Loss: 0.016981529718692823\n",
      "Epoch 9700, Loss: 0.016837836561846526\n",
      "Epoch 9800, Loss: 0.016696695126974147\n",
      "Epoch 9900, Loss: 0.016558035987394117\n",
      "Epoch 10000, Loss: 0.016421792260401656\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(epoch=10000)\n",
    "model.train(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        26\n",
      "   macro avg       1.00      1.00      1.00        26\n",
      "weighted avg       1.00      1.00      1.00        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled, threshold=0.5)\n",
    "print(classification_report(y_test, y_pred)) #đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
